{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, base_path,  video_names, violation_type, annotation_file, fragment_length=50, overlap=100, transform=None, ):\n",
    "        self.base_path = base_path\n",
    "        self.video_names = video_names\n",
    "        self.transform = transform\n",
    "        self.annotation_file = annotation_file\n",
    "        self.violation_type = violation_type\n",
    "        self.target_resolution = (360, 360)\n",
    "        self.fragment_length = fragment_length\n",
    "        self.overlap = overlap\n",
    "        sum_frame = self.sum_frames_in_folder()\n",
    "        self.len = sum_frame // self.fragment_length + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return next(self.get_fragment(idx))\n",
    "\n",
    "    def get_fragment(self, idx):\n",
    "        for video_name in self.video_names:\n",
    "            with open(self.annotation_file, 'r', encoding='utf-8') as f:\n",
    "                annotations = json.load(f)\n",
    "            cap = cv2.VideoCapture(os.path.join(self.base_path,video_name))\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "            intervals = []\n",
    "            for annotation in annotations:\n",
    "                if annotation[\"name\"] == video_name and self.violation_type == annotation['type']:\n",
    "                    intervals.append((self.time_to_seconds(annotation[\"start\"]), self.time_to_seconds(annotation['end'])))\n",
    "            if len(intervals) == 0:\n",
    "                continue\n",
    "            labels = np.zeros(total_frames)\n",
    "            for start_time, end_time in intervals:\n",
    "                start_frame = int(frame_rate * start_time)\n",
    "                end_frame = int(frame_rate * end_time)\n",
    "                labels[start_frame:end_frame+1] = 1\n",
    "\n",
    "            #if start_frame - 20 >= 0:\n",
    "                #cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame-20)\n",
    "            #else:\n",
    "                #cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            frames = []\n",
    "            frame_count = 0\n",
    "            for i in range(total_frames):\n",
    "                frame_count += 1\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                frame = cv2.resize(frame, self.target_resolution)\n",
    "                frame = torch.from_numpy(frame).float()\n",
    "                frames.append(frame)\n",
    "\n",
    "                if frame_count >= self.fragment_length:\n",
    "                    video_fragment = torch.stack(frames)\n",
    "                    yield video_fragment, torch.from_numpy(labels[i+1-frame_count:i+1]).float()\n",
    "                    frames = []\n",
    "                    frame_count = 0\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "    def count_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        return total_frames\n",
    "\n",
    "    def sum_frames_in_folder(self):\n",
    "        total_frames = 0\n",
    "        for file in self.video_names:\n",
    "            video_path = os.path.join(self.base_path, file)\n",
    "            total_frames += self.count_frames(video_path)\n",
    "        return total_frames\n",
    "\n",
    "    def time_to_seconds(self, time_str):\n",
    "        \"\"\"Преобразует строку формата MM:SS в количество секунд\"\"\"\n",
    "        minutes, seconds = map(int, time_str.split(':'))\n",
    "        return minutes * 60 + seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к папке с видео\n",
    "video_dir = '../data/train'\n",
    "\n",
    "# Путь к файлу разметки\n",
    "annotation_file = '../data/train/разметка.json'\n",
    "output_path = \"../data/подлезание/\"\n",
    "\n",
    "# Путь к папке, куда будут сохранены последовательности\n",
    "violation_type = \"Подлезание под вагоны стоящего состава\"\n",
    "target_resolution = (640, 480)\n",
    "\n",
    "# Подготовка данных\n",
    "#X, y = prepare_dataset(video_dir, annotation_file, violation_type, target_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vide_names = os.listdir(video_dir)\n",
    "\n",
    "# Задаем долю для тестовой выборки (например, 20%)\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Вычисляем количество элементов для тестовой выборки\n",
    "num_test = int(len(vide_names) * test_ratio)\n",
    "\n",
    "# Рандомно выбираем элементы для тестовой выборки\n",
    "test_set = random.sample(vide_names, num_test)\n",
    "\n",
    "# Формируем обучающую выборку\n",
    "train_set = [x for x in vide_names if x not in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomVideoDataset(video_dir, train_set, violation_type, annotation_file)\n",
    "val_dataset = CustomVideoDataset(video_dir, test_set, violation_type, annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\\ntrain_dataset = CustomDataset(X_train, y_train)\\ntest_dataset = CustomDataset(X_test, y_test)\\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\\nval_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lable = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 360, 360, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 24, 3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(24, 48, 3)\n",
    "            )\n",
    "        #self.cnn = nn.Sequential(*list(self.cnn.children())[:-2])  # Убираем полносвязный слой\n",
    "\n",
    "        self.lstm_input_size = self._get_conv_output_size((3, 360, 360))\n",
    "        self.lstm = nn.LSTM(self.lstm_input_size, 32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "\n",
    "    def _get_conv_output_size(self, shape):\n",
    "        o = self.cnn(torch.zeros(1, *shape))\n",
    "        return int(torch.prod(torch.tensor(o.size())))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, h, w, c = x.size()\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = CNNLSTM(num_classes=1)  # Одна выходная нейрона для вероятности нарушения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Сколько эпох ждать после последнего улучшения метрики\n",
    "            verbose (bool): Выводить сообщения о каждой проверке (по умолчанию False)\n",
    "            delta (float): Минимальное изменение для улучшения метрики\n",
    "            path (str): Путь для сохранения модели\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Сохранение модели, если валидационная потеря уменьшилась'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mievst\\anaconda3\\envs\\DFO\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 50])) that is different to the input size (torch.Size([1, 50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.285660  [    0/ 4527]\n",
      "loss: 0.001165  [   10/ 4527]\n",
      "loss: 0.000233  [   20/ 4527]\n",
      "loss: 0.000286  [   30/ 4527]\n",
      "loss: 0.000321  [   40/ 4527]\n",
      "loss: 0.000307  [   50/ 4527]\n",
      "loss: 0.000290  [   60/ 4527]\n",
      "loss: 0.000368  [   70/ 4527]\n",
      "loss: 0.000680  [   80/ 4527]\n",
      "loss: 0.007860  [   90/ 4527]\n",
      "loss: 0.000161  [  100/ 4527]\n",
      "loss: 0.000012  [  110/ 4527]\n",
      "loss: 0.000096  [  120/ 4527]\n",
      "loss: 0.000043  [  130/ 4527]\n",
      "loss: 0.000022  [  140/ 4527]\n",
      "loss: 0.000015  [  150/ 4527]\n",
      "loss: 0.000014  [  160/ 4527]\n",
      "loss: 0.000014  [  170/ 4527]\n",
      "loss: 0.000014  [  180/ 4527]\n",
      "loss: 0.000014  [  190/ 4527]\n",
      "loss: 0.000014  [  200/ 4527]\n",
      "loss: 0.000014  [  210/ 4527]\n",
      "loss: 0.000014  [  220/ 4527]\n",
      "loss: 0.000014  [  230/ 4527]\n",
      "loss: 0.000014  [  240/ 4527]\n",
      "loss: 0.000014  [  250/ 4527]\n",
      "loss: 0.000014  [  260/ 4527]\n",
      "loss: 0.000014  [  270/ 4527]\n",
      "loss: 0.000013  [  280/ 4527]\n",
      "loss: 0.000013  [  290/ 4527]\n",
      "loss: 0.000013  [  300/ 4527]\n",
      "loss: 0.000013  [  310/ 4527]\n",
      "loss: 0.000013  [  320/ 4527]\n",
      "loss: 0.000013  [  330/ 4527]\n",
      "loss: 0.000013  [  340/ 4527]\n",
      "loss: 0.000013  [  350/ 4527]\n",
      "loss: 0.000013  [  360/ 4527]\n",
      "loss: 0.000013  [  370/ 4527]\n",
      "loss: 0.000013  [  380/ 4527]\n",
      "loss: 0.000013  [  390/ 4527]\n",
      "loss: 0.000013  [  400/ 4527]\n",
      "loss: 0.000013  [  410/ 4527]\n",
      "loss: 0.000013  [  420/ 4527]\n",
      "loss: 0.000013  [  430/ 4527]\n",
      "loss: 0.000013  [  440/ 4527]\n",
      "loss: 0.000013  [  450/ 4527]\n",
      "loss: 0.000013  [  460/ 4527]\n",
      "loss: 0.000013  [  470/ 4527]\n",
      "loss: 0.000013  [  480/ 4527]\n",
      "loss: 0.000013  [  490/ 4527]\n",
      "loss: 0.000013  [  500/ 4527]\n",
      "loss: 0.000012  [  510/ 4527]\n",
      "loss: 0.000012  [  520/ 4527]\n",
      "loss: 0.000012  [  530/ 4527]\n",
      "loss: 0.000012  [  540/ 4527]\n",
      "loss: 0.000012  [  550/ 4527]\n",
      "loss: 0.000012  [  560/ 4527]\n",
      "loss: 0.000012  [  570/ 4527]\n",
      "loss: 0.000012  [  580/ 4527]\n",
      "loss: 0.000012  [  590/ 4527]\n",
      "loss: 0.000012  [  600/ 4527]\n",
      "loss: 0.000012  [  610/ 4527]\n",
      "loss: 0.000012  [  620/ 4527]\n",
      "loss: 0.000012  [  630/ 4527]\n",
      "loss: 0.000012  [  640/ 4527]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m---> 21\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     23\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Инициализация модели, функции потерь и оптимизатора\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Параметры ранней остановки\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "# Цикл обучения\n",
    "# Lists to store metrics for plotting\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    size = len(train_dataset)\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(data)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            predictions.extend(output.argmax(dim=1).tolist())\n",
    "            targets.extend(target.tolist())\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    precision = precision_score(targets, predictions)\n",
    "    recall = recall_score(targets, predictions)\n",
    "    f1 = f1_score(targets, predictions)\n",
    "\n",
    "    # Append metrics to lists for plotting\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.6f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epoch+2), accuracy_list, label='Accuracy')\n",
    "    plt.plot(range(1, epoch+2), precision_list, label='Precision')\n",
    "    plt.plot(range(1, epoch+2), recall_list, label='Recall')\n",
    "    plt.plot(range(1, epoch+2), f1_list, label='F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Early stopping condition\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_timestamps(predictions, threshold=0.5, frame_rate=30, window_size=1):\n",
    "    \"\"\"\n",
    "    Получение таймкодов нарушений на основе предсказаний модели.\n",
    "\n",
    "    predictions: np.array, форма [seq_length]\n",
    "        Предсказанные вероятности наличия нарушения для каждого окна кадров.\n",
    "    threshold: float\n",
    "        Пороговое значение для определения нарушения.\n",
    "    frame_rate: int\n",
    "        Количество кадров в секунду.\n",
    "    window_size: int\n",
    "        Количество кадров в одном окне.\n",
    "\n",
    "    Возвращает список кортежей (start_time, end_time) для каждого обнаруженного нарушения.\n",
    "    \"\"\"\n",
    "    timestamps = []\n",
    "    in_violation = False\n",
    "    start_time = None\n",
    "\n",
    "    for i, prob in enumerate(predictions):\n",
    "        if prob >= threshold and not in_violation:\n",
    "            in_violation = True\n",
    "            start_time = i * window_size / frame_rate\n",
    "        elif prob < threshold and in_violation:\n",
    "            in_violation = False\n",
    "            end_time = (i + 1) * window_size / frame_rate\n",
    "            timestamps.append((start_time, end_time))\n",
    "\n",
    "    # Если нарушение продолжается до конца видео\n",
    "    if in_violation:\n",
    "        end_time = len(predictions) * window_size / frame_rate\n",
    "        timestamps.append((start_time, end_time))\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "# Пример предсказаний (для упрощения, генерируем случайные данные)\n",
    "predictions = np.random.rand(10)  # 10 окон кадров\n",
    "predictions[2:4] = 0.7  # Пример нарушения\n",
    "predictions[7:8] = 0.8  # Пример нарушения\n",
    "\n",
    "# Получение таймкодов нарушений\n",
    "timestamps = get_timestamps(predictions, threshold=0.5, frame_rate=30, window_size=1)\n",
    "print(timestamps)  # Ожидаемый выход: [(start_time, end_time), ...]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DFO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
